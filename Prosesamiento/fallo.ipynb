{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "#from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han guardado correctamente 'UsersItems.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Función para cargar y corregir cada línea del archivo JSON original\n",
    "def cargar_y_corregir_linea(linea):\n",
    "    # Cambia comillas simples por comillas dobles\n",
    "    linea_corregida = linea.replace(\"'\", '\"')\n",
    "    return json.loads(linea_corregida)\n",
    "\n",
    "# Lista para almacenar los datos desanidados\n",
    "data_list = []\n",
    "\n",
    "# Lee el archivo JSON original, corrige y procesa cada línea\n",
    "with open(r'C:\\Users\\gaba_\\Desktop\\proyectos Henrry\\pi1-steam\\pi1-venv\\datasets\\UserItems.json', 'r', encoding='utf-8') as archivo_json:\n",
    "    for linea in archivo_json:\n",
    "        try:\n",
    "            entrada = cargar_y_corregir_linea(linea)\n",
    "            # Obtén datos del usuario\n",
    "            user_id = entrada['user_id']\n",
    "            items_count = entrada['items_count']\n",
    "            steam_id = entrada['steam_id']\n",
    "            user_url = entrada['user_url']\n",
    "            # Itera sobre cada ítem\n",
    "            for item in entrada['items']:\n",
    "                # Crea un diccionario para cada ítem y agrégalo a la lista\n",
    "                item_dict = {\n",
    "                    'user_id': user_id,\n",
    "                    'items_count': items_count,\n",
    "                    'steam_id': steam_id,\n",
    "                    'user_url': user_url,\n",
    "                    'item_id': item['item_id'],\n",
    "                    'item_name': item['item_name'],\n",
    "                    'playtime_forever': item['playtime_forever'],\n",
    "                    'playtime_2weeks': item['playtime_2weeks']\n",
    "                }\n",
    "                data_list.append(item_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            None\n",
    "\n",
    "# Crea el DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Guarda el DataFrame en un nuevo archivo CSV\n",
    "df.to_csv('UsersItems.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Se han guardado correctamente 'UsersItems.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo .json descomprimido y guardado como: ../datasets/data comprimida/UserReviews.json\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo .json.gz\n",
    "archivo_json_gz = '../datasets/data comprimida/user_reviews.json.gz'\n",
    "# Ruta para guardar el archivo .json resultante\n",
    "archivo_json = '../datasets/data comprimida/UserReviews.json'\n",
    "\n",
    "# Descomprimir el archivo .json.gz\n",
    "with gzip.open(archivo_json_gz, 'rb') as f_in:\n",
    "    with open(archivo_json, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Archivo .json descomprimido y guardado como:\", archivo_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_9516\\1595959858.py:69: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['posted'] = pd.to_datetime(df['posted'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El análisis de sentimiento ha sido  guardado en 'UserReviews.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Función para corregir y cargar cada línea del archivo 'australian_user_reviews.json' original\n",
    "def cargar_y_corregir_linea(linea):\n",
    "    # Reemplaza las comillas simples con dobles y corrige valores booleanos\n",
    "    linea_corregida = linea.replace(\"'\", '\"').replace('True', 'true').replace('False', 'false')\n",
    "    return json.loads(linea_corregida)\n",
    "\n",
    "# Lista para almacenar los datos desanidados\n",
    "data_list = []\n",
    "\n",
    "# Expresión regular para extraer números de la columna 'funny'\n",
    "numero_regex = re.compile(r'\\d+')\n",
    "\n",
    "# Lee el archivo 'australian_user_reviews.json' original, corrige y procesa cada línea\n",
    "with open('../datasets/UserReviews.json', 'r', encoding='utf-8') as archivo_json:\n",
    "    for linea in archivo_json:\n",
    "        try:\n",
    "            entrada = cargar_y_corregir_linea(linea)\n",
    "            user_id = entrada['user_id']\n",
    "            user_url = entrada['user_url']\n",
    "            # Iteración sobre cada reseña\n",
    "            for reseña in entrada['reviews']:\n",
    "                # Extrae el número de la columna 'funny'\n",
    "                funny_valor = re.search(numero_regex, reseña.get('funny', ''))\n",
    "                if funny_valor:\n",
    "                    funny = int(funny_valor.group())\n",
    "                else:\n",
    "                    funny = None\n",
    "                \n",
    "                # Elimina 'Posted' de la columna 'posted'\n",
    "                posted = reseña.get('posted', '').replace('Posted ', '', 1)\n",
    "\n",
    "                reseña_dict = {\n",
    "                    'user_id': user_id,\n",
    "                    'user_url': user_url,\n",
    "                    'funny': funny,\n",
    "                    'posted': posted,\n",
    "                    'item_id': int(reseña.get('item_id', '')),  # Convierte a entero\n",
    "                    'helpful': reseña.get('helpful', ''),\n",
    "                    'recommend': reseña.get('recommend', ''),\n",
    "                    'review': reseña.get('review', '')  # Mantiene el texto original de la review\n",
    "                }\n",
    "                data_list.append(reseña_dict)\n",
    "        except json.JSONDecodeError as e:\n",
    "            None\n",
    "\n",
    "# Crea el DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    " \n",
    "\n",
    "# Función para analizar el sentimiento de la reseña\n",
    "def analyze_sentiment(review):\n",
    "    if pd.isna(review) or not isinstance(review, str):\n",
    "        return 1  # Sentimiento neutral para reseñas faltantes o no legibles\n",
    "    else:\n",
    "        # Se emplea TextBlob para calcular la polaridad del sentimiento\n",
    "        polarity = TextBlob(review).sentiment.polarity\n",
    "        if polarity < 0:\n",
    "            return 0  # Sentimiento negativo\n",
    "        elif polarity == 0:\n",
    "            return 1  # Sentimiento neutral\n",
    "        else:\n",
    "            return 2  # Sentimiento positivo\n",
    "\n",
    "# Aplica la función analyze_sentiment a cada reseña y crea la nueva columna 'sentiment_analysis'\n",
    "df['sentiment_analysis'] = df['review'].apply(analyze_sentiment)\n",
    "\n",
    "# Convierte la columna 'posted' a tipo datetime\n",
    "df['posted'] = pd.to_datetime(df['posted'], errors='coerce')\n",
    "\n",
    "# Elimina la columna original 'review'\n",
    "df.drop(columns=['review'], inplace=True)\n",
    "\n",
    "# Guarda el DataFrame modificado en un nuevo archivo CSV\n",
    "df.to_csv('UserReviews.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"El análisis de sentimiento ha sido  guardado en 'UserReviews.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"datasets/nuevo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../datasets/UserReviews.csv')\n",
    "df3 = pd.read_csv('../datasets/UsersItems.csv')\n",
    "df4 = pd.read_csv('../datasets/datos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_juegos(df_datos, df_reviews, id_producto):\n",
    "    # Selecciona y renombra columnas necesarias\n",
    "    df_auc = df_datos[['genres', 'tags', 'specs', 'id']]\n",
    "    df_auc2 = df_reviews[['item_id', 'recommend']].astype({'item_id': 'float'}).rename(columns={'item_id': 'id'})\n",
    "    \n",
    "    # Combina dataframes en base al ID\n",
    "    df_auc3 = pd.merge(df_auc2, df_auc, on='id').reset_index(drop=True)\n",
    "    df_auc3 = df_auc3[df_auc3['recommend'] != False]\n",
    "    \n",
    "    # Combina columnas de texto y vectoriza\n",
    "    df_auc3['combined'] = df_auc3[['genres', 'tags']].fillna('').agg(' '.join, axis=1)\n",
    "    \n",
    "    # Configura el vectorizador TF-IDF\n",
    "    vec = TfidfVectorizer(max_features=1000)  # Limita el número de características para mejorar el rendimiento\n",
    "    vec_matrix = vec.fit_transform(df_auc3['combined'])\n",
    "    \n",
    "    # Calcula la similitud del coseno\n",
    "    coseno = cosine_similarity(vec_matrix, dense_output=False)\n",
    "    \n",
    "    # Busca el juego en el DataFrame\n",
    "    juego_en_data = df_datos[df_datos['id'] == id_producto]\n",
    "    if not juego_en_data.empty:\n",
    "        juego_indice = juego_en_data.index[0]\n",
    "        # Obtiene los juegos similares\n",
    "        juegos_similares = coseno[juego_indice].toarray().flatten()\n",
    "        # Ordena de mayor a menor\n",
    "        juegos_mas_similares = np.argsort(-juegos_similares)\n",
    "        # Obtiene los 6 primeros\n",
    "        top_5_juegos = df_datos.loc[juegos_mas_similares[1:6], 'app_name']\n",
    "        # Convierte a lista\n",
    "        top_5_juegos_mostrar = top_5_juegos.tolist()\n",
    "        # Nombre del juego original\n",
    "        nombre_del_juego = df_datos.loc[juego_indice, 'app_name']\n",
    "        \n",
    "        return 'Los 5 juegos recomendados para el id {} ({}) son: {}'.format(id_producto, nombre_del_juego, top_5_juegos_mostrar)\n",
    "    else:\n",
    "        return 'El juego no está en la base de datos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 5 juegos recomendados para el id 752360.0 (fallen mage) son: ['ghostbusters: sanctum of slime challenge pack dlc', 'blue reflection - bath towels set c (lime, fumio, chihiro)', 'the 9th day:第九日', 'crazy machines 2: invaders from space, 2nd wave dlc', 'labyrinth - starter pack']\n"
     ]
    }
   ],
   "source": [
    "print(recomendar_juegos(df4,df2,752360.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Los 5 juegos recomendados para el id 752360.0 (fallen mage) son: ['ghostbusters: sanctum of slime challenge pack dlc', 'blue reflection - bath towels set c (lime, fumio, chihiro)', 'the 9th day:第九日', 'crazy machines 2: invaders from space, 2nd wave dlc', 'labyrinth - starter pack']\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc = df4[['genres','tags','specs','id']]\n",
    "df_auc2= df2[['item_id','recommend']]\n",
    "df_auc2['item_id'] =df_auc2['item_id'].astype(float)\n",
    "df_auc2.rename(columns={'item_id':'id'},inplace=True)\n",
    "df_auc3=pd.merge(df_auc2,df_auc,on='id').reset_index(drop=True)\n",
    "df_auc3=df_auc3[(df_auc3['recommend']!=False)]\n",
    "#transformo las columnas a srt para poder tabajarlas\n",
    "#generos=df_auc3['genres'].astype(str)\n",
    "#tags=df_auc3['tags'].astype(str)\n",
    "#specs=df_auc3['specs'].astype(str)\n",
    "vec=TfidfVectorizer()\n",
    "vec_matrix1= vec.fit_transform(df_auc3['genres'])\n",
    "#vec_matrix2= vec.fit_transform(df_auc3['tags'])\n",
    "#vec_matrix3= vec.fit_transform(specs)\n",
    "\n",
    "matrix_completa=np.column_stack([vec_matrix1.toarray()])\n",
    "#calculo la similituid del coseno\n",
    "coseno=cosine_similarity(matrix_completa)\n",
    "#coseno=linear_kernel(matrix_completa,matrix_completa) \n",
    "id_producto=304930.0\n",
    "#buaca el juego en el dataFrame\n",
    "juego_en_data= df4[df4['id']== id_producto]\n",
    "if not juego_en_data.empty:\n",
    "    juego_indice=juego_en_data.index[0]\n",
    "    #obtengo los similares\n",
    "    juegos_similares=coseno[juego_indice]\n",
    "    #los ordeno de mayor a menor\n",
    "    juegos_mas_similares=np.argsort(-juegos_similares)\n",
    "    #obtengo los 6 primeros\n",
    "    top_5_juegos=df4.loc[juegos_mas_similares[0:6],'app_name']\n",
    "    #los combierto en lista \n",
    "    top_5_juegos_mostrar=top_5_juegos.to_numpy().tolist()\n",
    "    #tomo quito el primer valor para guardarlo en una variable para mostrar el nombre del juego que ingrese por id\n",
    "    nombre_del_juego= top_5_juegos_mostrar.pop(0)\n",
    "    a= ('los 5 juegos recomendados para el id {} ({}) son: {}'.format(id_producto,nombre_del_juego,top_5_juegos_mostrar) )\n",
    "    #return print(f'los 5 juegos recomendados para el id {id_producto} ({nombre_del_juego}) son: {top_5_juegos_mostrar}' )\n",
    "else:\n",
    "    a='el juego no esta en la base de datos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=TfidfVectorizer(min_df=5,max_df=0.98)\n",
    "vec_matrix1= vec.fit_transform(df_auc3['genres'])\n",
    "\n",
    "\n",
    "matrix_completa=np.column_stack(vec_matrix1.toarray())\n",
    "#calculo la similituid del coseno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "coseno2= linear_kernel(matrix_completa,matrix_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "coseno=cosine_similarity(matrix_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los 5 juegos recomendados para el id 304930.0 (crystalline) son: ['foresight ost', 'fantasy grounds - top down tokens - steampunk', 'beats fever - big apple', 'dead or alive 5 last round: core fighters add \"dead or alive 3 music\"', 'press x to not die - soundtrack']\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso todos precios a float, los que son F2P pasan a NaN\n",
    "df4.loc[:,'price']=pd.to_numeric(df4['price'], errors='coerce')\n",
    "#paso los NaN a 0\n",
    "df4.loc[:,'price']=df4['price'].fillna(0)\n",
    "#paso todo a minusculas\n",
    "df4.loc[:,'developer'] = df4['developer'].str.lower()\n",
    "#paso la columna a a formato de fecha\n",
    "df4['release_date'] = pd.to_datetime(df4['release_date'],errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "#transformo las columnas a srt para poder tabajarlas\n",
    "generos=df4['genres'].astype(str)\n",
    "tags=df4['tags'].astype(str)\n",
    "specs=df4['specs'].astype(str)\n",
    "\n",
    "\n",
    "vec=TfidfVectorizer()\n",
    "\n",
    "#creo matriz\n",
    "vec_matrix1= vec.fit_transform(generos)\n",
    "vec_matrix2= vec.fit_transform(tags)\n",
    "vec_matrix3= vec.fit_transform(specs)\n",
    "\n",
    "#uno las matrises\n",
    "matrix_completa=np.column_stack([vec_matrix1.toarray(),vec_matrix3.toarray(),vec_matrix2.toarray()])\n",
    "\n",
    "#calculo la similituid del coseno\n",
    "coseno=cosine_similarity(matrix_completa)\n",
    "id_producto=248820.0\n",
    "#buaca el juego en el dataFrame\n",
    "juego_en_data= df4[df4['id']== id_producto]\n",
    "\n",
    "if not juego_en_data.empty:\n",
    "    juego_indice=juego_en_data.index[0]\n",
    "    #obtengo los similares\n",
    "    juegos_similares= coseno[juego_indice]\n",
    "    #los ordeno de mayor a menor\n",
    "    juegos_mas_similares=np.argsort(-juegos_similares)\n",
    "    #obtengo los 6 primeros\n",
    "    top_5_juegos=df4.loc[juegos_mas_similares[0:6],'app_name']\n",
    "    #los combierto en lista \n",
    "    top_5_juegos_mostrar=top_5_juegos.to_numpy().tolist()\n",
    "    #tomo quito el primer valor para guardarlo en una variable para mostrar el nombre del juego que ingrese por id\n",
    "    nombre_del_juego= top_5_juegos_mostrar.pop(0)\n",
    "    a= (f'los 5 juegos recomendados para el id {id_producto} ({nombre_del_juego}) son: {top_5_juegos_mostrar}' )\n",
    "    #return print(f'los 5 juegos recomendados para el id {id_producto} ({nombre_del_juego}) son: {top_5_juegos_mostrar}' )\n",
    "else:\n",
    "    a='el juego no esta en la base de datos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_11668\\3363274597.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ra2['id']=df_ra2['id'].astype(int)\n",
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_11668\\3363274597.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ra.rename(columns={'item_id':'id'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "developer = 'Valve'\n",
    "df_ra=df2[['item_id','sentiment_analysis']]\n",
    "df_ra2=df4[['id','developer']]\n",
    "df_ra2['id']=df_ra2['id'].astype(int)\n",
    "df_ra.rename(columns={'item_id':'id'}, inplace=True)\n",
    "df_ra3=pd.merge(df_ra,df_ra2,on='id')\n",
    "df_ra3 = df_ra3.drop(columns='id')\n",
    "df_ra3 = df_ra3[(df_ra3['sentiment_analysis']!=1)]\n",
    "df_positivos = df_ra3[df_ra3['sentiment_analysis'] == 2].groupby('developer').size().reset_index(name='Positivos')\n",
    "df_negativos = df_ra3[df_ra3['sentiment_analysis'] == 0].groupby('developer').size().reset_index(name='Negativos')\n",
    "\n",
    "# Combinar los DataFrames\n",
    "df_final = pd.merge(df_positivos, df_negativos, on='developer', how='outer').fillna(0)\n",
    "df_final['Positivos'] = df_final['Positivos'].astype(int)\n",
    "df_final['Negativos'] = df_final['Negativos'].astype(int)\n",
    "df_salida=df_final[df_final['developer']==developer].reset_index()\n",
    "dic_salida = {developer : [f'Negative = {df_salida['Negativos'].loc[0]}' , f'Positive = {df_salida[\"Positivos\"].loc[0]}' ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Valve': ['Negative = 882', 'Positive = 3268']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>developer</th>\n",
       "      <th>Positivos</th>\n",
       "      <th>Negativos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Valve</td>\n",
       "      <td>3268</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     developer  Positivos  Negativos\n",
       "1116     Valve       3268        882"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>developer</th>\n",
       "      <th>Positivos</th>\n",
       "      <th>Negativos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 bit studios</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17-BIT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1C Company</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1C: Maddox Games</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22cans</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>tinyBuild</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>tobyfox</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>△○□× (Miwashiba)</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>インレ,Inre</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>橘子班</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             developer  Positivos  Negativos\n",
       "0       11 bit studios         16          4\n",
       "1               17-BIT          0          1\n",
       "2           1C Company          0          1\n",
       "3     1C: Maddox Games          1          0\n",
       "4               22cans          1          0\n",
       "...                ...        ...        ...\n",
       "1196         tinyBuild          2          0\n",
       "1197           tobyfox         32         13\n",
       "1198  △○□× (Miwashiba)          3          0\n",
       "1199          インレ,Inre          0          1\n",
       "1200               橘子班          0          1\n",
       "\n",
       "[1201 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def developer_reviews_analysis(desarrolador):\n",
    "    df_ra=df2[['item_id','sentiment_analysis']]\n",
    "    df_ra2=df4[['id','developer']]\n",
    "    df_ra2['id']=df_ra2['id'].astype(int)\n",
    "    df_ra.rename(columns={'item_id':'id'}, inplace=True)\n",
    "    df_ra3=pd.merge(df_ra,df_ra2,on='id')\n",
    "    df_ra3 = df_ra3.drop(columns='id')\n",
    "    df_ra3 = df_ra3[(df_ra3['sentiment_analysis']!=1)]\n",
    "    df_positivos = df_ra3[df_ra3['sentiment_analysis'] == 2].groupby('developer').size().reset_index(name='Positivos')\n",
    "    df_negativos = df_ra3[df_ra3['sentiment_analysis'] == 0].groupby('developer').size().reset_index(name='Negativos')\n",
    "\n",
    "    # Combinar los DataFrames\n",
    "    df_final = pd.merge(df_positivos, df_negativos, on='developer', how='outer').fillna(0)\n",
    "    df_final['Positivos'] = df_final['Positivos'].astype(int)\n",
    "    df_final['Negativos'] = df_final['Negativos'].astype(int)\n",
    "    df_salida=df_final[df_final['developer']==desarrolador].reset_index()\n",
    "    dic_salida = {developer : [f'Negative = {df_salida['Negativos'].loc[0]}' , f'Positive = {df_salida[\"Positivos\"].loc[0]}' ]}\n",
    "    return dic_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_11668\\2081564235.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ra2['id']=df_ra2['id'].astype(int)\n",
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_11668\\2081564235.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ra.rename(columns={'item_id':'id'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Valve': ['Negative = 882', 'Positive = 3268']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "developer_reviews_analysis('Valve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['release_date'] = pd.to_datetime(df4['release_date'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el genero\n",
    "genero='Adventure'\n",
    "lista_jugos=[]\n",
    "lista_ano=[]\n",
    "val= 0\n",
    "for i ,j , k in zip(df4['genres'],df4['release_date'].dt.year,df4['id']):\n",
    "    if genero in i: \n",
    "        try:\n",
    "            lista_ano.append(int(j))\n",
    "            lista_jugos.append(int(k))\n",
    "        except Exception as e:\n",
    "            val+=1\n",
    "df= pd.DataFrame({'item_id':lista_jugos,'año':lista_ano})\n",
    "lista_usuarios=[]\n",
    "lista_itesm=[]\n",
    "lista_horas=[]\n",
    "for i,j,k in zip(df3['user_id'],df3['item_id'],df3['playtime_forever']):\n",
    "    if j in lista_jugos:\n",
    "        lista_usuarios.append(i)\n",
    "        lista_itesm.append(j)\n",
    "        lista_horas.append(k)\n",
    "new_df = pd.DataFrame({'user_id':lista_usuarios,'item_id':lista_itesm,'playtime_forever':lista_horas})\n",
    "new_df = pd.merge(df, new_df, on='item_id')\n",
    "new_df=new_df.sort_values(by='playtime_forever', ascending=False)\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "new_df['playtime_forever'] = new_df['playtime_forever'].apply(lambda x: x / 60)\n",
    "new_df['playtime_forever'] = new_df['playtime_forever'].round().astype(int)\n",
    "new_df2 = new_df\n",
    "new_df2=new_df2.drop(columns=['item_id'])\n",
    "filtro_usuario=new_df2.groupby('user_id',)[['playtime_forever']].sum().reset_index()\n",
    "new_df3 =new_df2.groupby(['año','user_id']).agg({'playtime_forever': 'sum'}).reset_index()\n",
    "filtro_usuario=filtro_usuario.sort_values(by='playtime_forever', ascending=False)\n",
    "filtro_usuario.reset_index(drop=True, inplace=True)\n",
    "usuario=filtro_usuario['user_id'][0]\n",
    "new_filtrado=new_df3[new_df3['user_id']==usuario]\n",
    "new_filtrado = new_filtrado.sort_values(by='año', ascending=False)\n",
    "new_filtrado.reset_index(drop=True, inplace=True)\n",
    "lista_aux=[]\n",
    "for i,j in zip(new_filtrado['año'],new_filtrado['playtime_forever']):\n",
    "    dicc_aux={'Año':i,'Horas':j}\n",
    "    lista_aux.append(dicc_aux)\n",
    "diccionario_salida={f'Usuario con más horas jugadas para Género {genero}':usuario,\n",
    "                        'Horas jugadas':lista_aux}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_6256\\2276199116.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_auxi1['id'] = df_auxi1['id'].astype(int)\n",
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_6256\\2276199116.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_auxi1['Año']=df_auxi1['release_date'].dt.year\n",
      "C:\\Users\\gaba_\\AppData\\Local\\Temp\\ipykernel_6256\\2276199116.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_auxi2.rename(columns={'item_id':'id'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#df para trabajar y unir con el otro\n",
    "ano=2000\n",
    "df_auxi1= df4[['release_date','id','developer']]\n",
    "df_auxi1['id'] = df_auxi1['id'].astype(int)\n",
    "df_auxi1['Año']=df_auxi1['release_date'].dt.year\n",
    "df_auxi1=df_auxi1.drop(columns='release_date')\n",
    "#df para unir con el de arriba\n",
    "df_auxi2 = df2[['item_id','recommend','sentiment_analysis']]\n",
    "df_auxi2.rename(columns={'item_id':'id'}, inplace=True)\n",
    "df_auxi3 = pd.merge(df_auxi1,df_auxi2,on='id')\n",
    "#limpiar todo el df una vez unido\n",
    "df_auxi3.dropna(inplace=True)\n",
    "df_auxi3['Año'] = df_auxi3['Año'].astype(int)\n",
    "df_auxi3= df_auxi3[(df_auxi3['sentiment_analysis']!=0)&(df_auxi3['sentiment_analysis']!=1)]\n",
    "df_auxi3= df_auxi3[(df_auxi3['recommend']!=False)]\n",
    "df_auxi3.drop(columns='id',inplace=True)\n",
    "df_auxi3=df_auxi3.groupby(['developer','Año', 'recommend'])['sentiment_analysis'].sum().reset_index()\n",
    "df_auxi3['sentiment_analysis']= (df_auxi3['sentiment_analysis']/2).round().astype(int)\n",
    "df_auxi4 = df_auxi3[df_auxi3['Año']==ano]\n",
    "df_auxi4 = df_auxi4.sort_values(by='sentiment_analysis', ascending=False).reset_index(drop=True)\n",
    "try:\n",
    "    puesto_1=df_auxi4['developer'].loc[0]\n",
    "except Exception as e:\n",
    "    puesto_1 = 'Este puesto esta basio' \n",
    "try:\n",
    "    puesto_2=df_auxi4['developer'].loc[1]\n",
    "except Exception as e:\n",
    "    puesto_2 = 'Este puesto esta basio'\n",
    "try:\n",
    "    puesto_3=df_auxi4['developer'].loc[2]\n",
    "except Exception as e:\n",
    "    puesto_3 = 'Este puesto esta basio'\n",
    "dicc_salida = [{'Puesto 1':puesto_1},{'Puesto 2':puesto_2},{'Puesto 3':puesto_3}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auxi3= df_auxi3[(df_auxi3['sentiment_analysis']!=0)&(df_auxi3['sentiment_analysis']!=1)]\n",
    "df_auxi3= df_auxi3[(df_auxi3['recommend']!=False)]\n",
    "df_auxi3.drop(columns='id',inplace=True)\n",
    "df_auxi3=df_auxi3.groupby(['developer','Año', 'recommend'])['sentiment_analysis'].sum().reset_index()\n",
    "df_auxi3['sentiment_analysis']= (df_auxi3['sentiment_analysis']/2).round().astype(int)\n",
    "df_auxi4 = df_auxi3[df_auxi3['Año']==ano]\n",
    "df_auxi4 = df_auxi4.sort_values(by='sentiment_analysis', ascending=False).reset_index(drop=True)\n",
    "dicc_salida = [{'Puesto 1':df_auxi4['developer'].loc[0]},{'Puesto 2':df_auxi4['developer'].loc[1]},{'Puesto 3':df_auxi4['developer'].loc[2]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_developer_year(ano):\n",
    "   \n",
    "\n",
    "    df_auxi1= df4[['release_date','id','developer']]\n",
    "    df_auxi1['id'] = df_auxi1['id'].astype(int)\n",
    "    df_auxi1['Año']=df_auxi1['release_date'].dt.year\n",
    "    df_auxi1=df_auxi1.drop(columns='release_date')\n",
    "    #df para unir con el de arriba\n",
    "    df_auxi2 = df2[['item_id','recommend','sentiment_analysis']]\n",
    "    df_auxi2.rename(columns={'item_id':'id'}, inplace=True)\n",
    "    df_auxi3 = pd.merge(df_auxi1,df_auxi2,on='id')\n",
    "    #limpiar todo el df una vez unido\n",
    "    df_auxi3.dropna(inplace=True)\n",
    "    df_auxi3['Año'] = df_auxi3['Año'].astype(int)\n",
    "    df_auxi3= df_auxi3[(df_auxi3['sentiment_analysis']!=0)&(df_auxi3['sentiment_analysis']!=1)]\n",
    "    df_auxi3= df_auxi3[(df_auxi3['recommend']!=False)]\n",
    "    df_auxi3.drop(columns='id',inplace=True)\n",
    "    df_auxi3=df_auxi3.groupby(['developer','Año', 'recommend'])['sentiment_analysis'].sum().reset_index()\n",
    "    df_auxi3['sentiment_analysis']= (df_auxi3['sentiment_analysis']/2).round().astype(int)\n",
    "    df_auxi4 = df_auxi3[df_auxi3['Año']==ano]\n",
    "    df_auxi4 = df_auxi4.sort_values(by='sentiment_analysis', ascending=False).reset_index(drop=True)\n",
    "    dicc_salida = [{'Puesto 1':df_auxi4['developer'].loc[0]},{'Puesto 2':df_auxi4['developer'].loc[1]},{'Puesto 3':df_auxi4['developer'].loc[2]}]\n",
    "    return diccionario_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserForGenre1(genero):\n",
    "    # Asegurarse de que la fecha esté en formato datetime\n",
    "    df4['release_date'] = pd.to_datetime(df4['release_date'], errors='coerce')\n",
    "    \n",
    "    # Filtrar los juegos que pertenecen al género especificado\n",
    "    juegos_genero = df4[df4['genres'].str.contains(genero, na=False)]\n",
    "    \n",
    "    # Crear un DataFrame con los juegos filtrados y sus años de lanzamiento\n",
    "    df_juegos = juegos_genero[['id', 'release_date']].copy()\n",
    "    df_juegos['año'] = df_juegos['release_date'].dt.year\n",
    "    df_juegos = df_juegos.drop(columns=['release_date']).rename(columns={'id': 'item_id'})\n",
    "    \n",
    "    # Filtrar las jugadas correspondientes a los juegos del género\n",
    "    df_jugadas = df3[df3['item_id'].isin(df_juegos['item_id'])]\n",
    "    \n",
    "    # Unir los DataFrames de juegos y jugadas\n",
    "    new_df = pd.merge(df_juegos, df_jugadas, on='item_id')\n",
    "    \n",
    "    # Convertir playtime_forever de minutos a horas, redondear y convertir a entero\n",
    "    new_df['playtime_forever'] = (new_df['playtime_forever'] / 60).round().astype(int)\n",
    "    \n",
    "    # Eliminar la columna item_id\n",
    "    new_df = new_df.drop(columns=['item_id'])\n",
    "    \n",
    "    # Agrupar por usuario y sumar las horas jugadas\n",
    "    filtro_usuario = new_df.groupby('user_id')['playtime_forever'].sum().reset_index()\n",
    "    \n",
    "    # Ordenar el DataFrame por horas jugadas en forma descendente\n",
    "    filtro_usuario = filtro_usuario.sort_values(by='playtime_forever', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Obtener el usuario con más horas jugadas\n",
    "    usuario = filtro_usuario.loc[0, 'user_id']\n",
    "    \n",
    "    # Agrupar por año y usuario, sumando las horas jugadas\n",
    "    new_df_agrupado = new_df.groupby(['año', 'user_id'])['playtime_forever'].sum().reset_index()\n",
    "    \n",
    "    # Filtrar solo las filas del usuario con más horas jugadas\n",
    "    new_filtrado = new_df_agrupado[new_df_agrupado['user_id'] == usuario]\n",
    "    \n",
    "    # Ordenar el DataFrame por año en forma descendente\n",
    "    new_filtrado = new_filtrado.sort_values(by='año', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Crear una lista con los años y las horas jugadas\n",
    "    lista_aux = new_filtrado[['año', 'playtime_forever']].rename(columns={'playtime_forever': 'Horas'}).to_dict(orient='records')\n",
    "    \n",
    "    # Crear el diccionario de salida\n",
    "    diccionario_salida = {\n",
    "        f'Usuario con más horas jugadas para Género {genero}': usuario,\n",
    "        'Horas jugadas': lista_aux\n",
    "    }\n",
    "\n",
    "    return diccionario_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserForGenre(genero):\n",
    "    #el genero entra po parametro\n",
    "    #me aseguro que la fecha este en foramo data time\n",
    "    df4['release_date'] = pd.to_datetime(df4['release_date'], errors='coerce')\n",
    "    #listas para guardar los juegos y los años \n",
    "    lista_jugos=[]\n",
    "    lista_ano=[]\n",
    "    #variable que guarda los nulos\n",
    "    val= 0\n",
    "    #recorro el df4 (dataframe) para obtener los balores de que quiero guardar en las listas\n",
    "    for i ,j , k in zip(df4['genres'],df4['release_date'].dt.year,df4['id']):\n",
    "        if genero in i: \n",
    "            try:\n",
    "                lista_ano.append(int(j))\n",
    "                lista_jugos.append(int(k))\n",
    "            except Exception as e:\n",
    "                val+=1\n",
    "    #creo un df (dataFrame) de las listas de juegos y años\n",
    "    df= pd.DataFrame({'item_id':lista_jugos,'año':lista_ano})\n",
    "    #creo las listas para guardar los usuarios los juegos y los minutos par cada juego\n",
    "    lista_usuarios=[]\n",
    "    lista_itesm=[]\n",
    "    lista_horas=[]\n",
    "    #recorro el df3 (dataframe) para obtener los datos de los usuarios, juegos y minutos jugados para cada juego \n",
    "    for i,j,k in zip(df3['user_id'],df3['item_id'],df3['playtime_forever']):\n",
    "        if j in lista_jugos:\n",
    "            lista_usuarios.append(i)\n",
    "            lista_itesm.append(j)\n",
    "            lista_horas.append(k)\n",
    "    #creo new_df (dataframe) con los datos anteriores\n",
    "    new_df = pd.DataFrame({'user_id':lista_usuarios,'item_id':lista_itesm,'playtime_forever':lista_horas})\n",
    "    #uno los dos dataframes previamente creados por la columna item_id (juegos)\n",
    "    new_df = pd.merge(df, new_df, on='item_id')\n",
    "    #ordeno el dataframe por minutos jugados de forma decendiente\n",
    "    new_df=new_df.sort_values(by='playtime_forever', ascending=False)\n",
    "    #receteo el indice\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    #paso la colimna playtime_forever de minutos a horas\n",
    "    new_df['playtime_forever'] = new_df['playtime_forever'].apply(lambda x: x / 60)\n",
    "    #paso la colimna playtime_forever de float a int (de flotante a entero)\n",
    "    new_df['playtime_forever'] = new_df['playtime_forever'].round().astype(int)\n",
    "    #Borro la columna item_id\n",
    "    new_df=new_df.drop(columns=['item_id'])\n",
    "    #Creo un dataframe que agrupe los usuarios y sumo las horas jugadas  \n",
    "    filtro_usuario=new_df.groupby('user_id',)[['playtime_forever']].sum().reset_index()\n",
    "    #Ordeno el dataframe de forma decendiente\n",
    "    filtro_usuario=filtro_usuario.sort_values(by='playtime_forever', ascending=False)\n",
    "    #reseteo el indice\n",
    "    filtro_usuario.reset_index(drop=True, inplace=True)\n",
    "    #obtengo el usuaro con mas oras jugadas\n",
    "    usuario=filtro_usuario['user_id'][0]\n",
    "    #agrupo los dataframe para que mueste cuantas horas tiene cada usuario por año y receteo el indice\n",
    "    new_df3 =new_df.groupby(['año','user_id']).agg({'playtime_forever': 'sum'}).reset_index()\n",
    "    #creo un dataframe que tiene solo al usuario con mas horas jugadas \n",
    "    new_filtrado=new_df3[new_df3['user_id']==usuario]\n",
    "    #Ordeno el dataframe por año\n",
    "    new_filtrado = new_filtrado.sort_values(by='año', ascending=False)\n",
    "    #reseteo el indice\n",
    "    new_filtrado.reset_index(drop=True, inplace=True)\n",
    "    #Creo una lista para guardar los años y las horas jugadas\n",
    "    lista_aux=[]\n",
    "    #recorro el dataframe, obtengo el año y oras jugas\n",
    "    for i,j in zip(new_filtrado['año'],new_filtrado['playtime_forever']):\n",
    "        dicc_aux={'Año':i,'Horas':j} #Guardo en un diccionario\n",
    "        lista_aux.append(dicc_aux) #Agrego los datos a la lista\n",
    "    #Creo el diccionario que deve devolver la funcion\n",
    "    diccionario_salida={f'Usuario con más horas jugadas para Género {genero}':usuario,\n",
    "                            'Horas jugadas':lista_aux}\n",
    "\n",
    "    return diccionario_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista1 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>items_count</th>\n",
       "      <th>steam_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>playtime_forever</th>\n",
       "      <th>playtime_2weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76561198070234207</td>\n",
       "      <td>33</td>\n",
       "      <td>76561198070234207</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561198070...</td>\n",
       "      <td>43110</td>\n",
       "      <td>Metro 2033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76561198070234207</td>\n",
       "      <td>33</td>\n",
       "      <td>76561198070234207</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561198070...</td>\n",
       "      <td>105600</td>\n",
       "      <td>Terraria</td>\n",
       "      <td>15590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76561198070234207</td>\n",
       "      <td>33</td>\n",
       "      <td>76561198070234207</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561198070...</td>\n",
       "      <td>38830</td>\n",
       "      <td>CrimeCraft GangWars</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id  items_count           steam_id  \\\n",
       "0  76561198070234207           33  76561198070234207   \n",
       "1  76561198070234207           33  76561198070234207   \n",
       "2  76561198070234207           33  76561198070234207   \n",
       "\n",
       "                                            user_url  item_id  \\\n",
       "0  http://steamcommunity.com/profiles/76561198070...    43110   \n",
       "1  http://steamcommunity.com/profiles/76561198070...   105600   \n",
       "2  http://steamcommunity.com/profiles/76561198070...    38830   \n",
       "\n",
       "             item_name  playtime_forever  playtime_2weeks  \n",
       "0           Metro 2033                 0                0  \n",
       "1             Terraria             15590                0  \n",
       "2  CrimeCraft GangWars                 1                0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso todos precios a float, los que son F2P pasan a NaN\n",
    "df4.loc[:,'price']=pd.to_numeric(df4['price'], errors='coerce')\n",
    "#paso los NaN a 0\n",
    "df4.loc[:,'price']=df4['price'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsi=0\n",
    "rno=0\n",
    "total=0\n",
    "for i, j in zip(df2['user_id'],df2['recommend']):\n",
    "    if i == '76561198089393905':\n",
    "        if j== True:\n",
    "            rsi+=1\n",
    "            total+=1\n",
    "        else:\n",
    "            rno+=1\n",
    "            total+=1\n",
    "porsentaje = int((rsi/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_items=0\n",
    "for i, j in zip(df3['user_id'],df3['items_count']):\n",
    "    if i == '76561198070234207':\n",
    "        total_items= j\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porsentaje = int((rsi/total)*100)\n",
    "porsentaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1=[]\n",
    "#Filtra por usuario\n",
    "for i,j in zip(df3['user_id'],df3['item_id']) :\n",
    "    if i == '76561198134938809':\n",
    "        lista1.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_data(user_id):\n",
    "    lista=[]\n",
    "    #Filtra por usuario\n",
    "    for i,j in zip(df3['user_id'],df3['item_id']) :\n",
    "        if i == user_id:\n",
    "            lista.append(j)\n",
    "    #Obtiene el presio de los juegos que tenga y los suma\n",
    "    precios= 0\n",
    "    for i, j in zip(df4['id'],df4['price']):\n",
    "        if i in lista:\n",
    "            precios+=j \n",
    "    dinero_gastado = f'{int(precios)} USD'\n",
    "    #Obtiene la contidad de recomendaciones\n",
    "    rsi=0\n",
    "    rno=0\n",
    "    total=0\n",
    "    for i, j in zip(df2['user_id'],df2['recommend']):\n",
    "        if i == user_id:\n",
    "            if j== True:\n",
    "                rsi+=1\n",
    "                total+=1\n",
    "            else:\n",
    "                rno+=1\n",
    "                total+=1\n",
    "    if total > 0:\n",
    "        porsentaje = int((rsi/total)*100)\n",
    "    else:\n",
    "        porsentaje = 0\n",
    "    porsentaje = f'{porsentaje}%'\n",
    "    #Obtiene la cantidad de items\n",
    "    total_items=0\n",
    "    for i, j in zip(df3['user_id'],df3['items_count']):\n",
    "        if i == user_id:\n",
    "            total_items= j\n",
    "        if total_items > 0:\n",
    "            break\n",
    "    #retorno de la funcion en formato diccionario\n",
    "    dic1 ={'usuario': user_id,\n",
    "           'Dinero gastado': dinero_gastado,\n",
    "           'Porsentaje de recomendacion':porsentaje,\n",
    "           'Cantidad de items': total_items\n",
    "           }\n",
    "    return dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usuario': 'lilim',\n",
       " 'Dinero gastado': '0 USD',\n",
       " 'Porsentaje de recomendacion': '33%',\n",
       " 'Cantidad de items': 0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pepe= user_data('lilim')\n",
    "pepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta= 'data comprimida/user_reviews.json.gz'\n",
    "steam_list=[] # lista de dattos\n",
    "\n",
    "with gzip.open(ruta,'rt',encoding='utf-8') as archivo:\n",
    "    for fila in archivo:\n",
    "        datos=json.loads(fila)\n",
    "        steam_list.append(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el archivo JSON modificado\n",
    "data = pd.read_json(\"datasets/nuevo.json\")\n",
    "\n",
    "# Si quieres acceder a los items de cada usuario\n",
    "items = data['items'].explode().apply(pd.Series)\n",
    "\n",
    "print(data)\n",
    "print(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al leer el archivo JSON: Expecting ',' delimiter: line 1 column 1394 (char 1393)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def formatear_json(archivo_entrada, archivo_salida):\n",
    "    \"\"\"\n",
    "    Formatea un archivo JSON para que pueda ser leído correctamente por pandas.\n",
    "    \n",
    "    :param archivo_entrada: Ruta al archivo JSON de entrada.\n",
    "    :param archivo_salida: Ruta al archivo JSON de salida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer el contenido del archivo JSON de entrada\n",
    "        with open(archivo_entrada, 'r', encoding='utf-8') as file:\n",
    "            contenido = json.load(file)\n",
    "        \n",
    "        # Formatear el contenido adecuadamente\n",
    "        datos_formateados = []\n",
    "        for usuario in contenido:\n",
    "            usuario_data = {\n",
    "                \"user_id\": usuario.get(\"user_id\", \"\"),\n",
    "                \"items_count\": usuario.get(\"items_count\", 0),\n",
    "                \"steam_id\": usuario.get(\"steam_id\", \"\"),\n",
    "                \"user_url\": usuario.get(\"user_url\", \"\"),\n",
    "                \"items\": usuario.get(\"items\", [])\n",
    "            }\n",
    "            datos_formateados.append(usuario_data)\n",
    "        \n",
    "        # Guardar el contenido formateado en el archivo de salida\n",
    "        with open(archivo_salida, 'w', encoding='utf-8') as file:\n",
    "            json.dump(datos_formateados, file, indent=4)\n",
    "        \n",
    "        print(f\"Archivo formateado guardado en {archivo_salida}\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error al leer el archivo JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Se produjo un error: {e}\")\n",
    "\n",
    "# Usar la función para formatear el archivo proporcionado\n",
    "archivo_entrada = 'datasets/nuevo.json'  # Cambia esta ruta por la ruta de tu archivo\n",
    "archivo_salida = 'datasets/nuevo_modificado.json'  # Cambia esta ruta por donde quieres guardar el archivo modificado\n",
    "\n",
    "formatear_json(archivo_entrada, archivo_salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo JSON corregido guardado en: datasets/australian_users_items_corrected.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Ruta del archivo JSON original\n",
    "file_path = 'datasets/australian_users_items.json'\n",
    "\n",
    "# Leer el archivo con comillas simples\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_content = file.read()\n",
    "\n",
    "# Expresión regular para reemplazar comillas simples por comillas dobles en las claves y valores del JSON\n",
    "json_corrected = re.sub(r\"(?<!\\\\)'\", '\"', json_content)\n",
    "\n",
    "# Guardar el JSON corregido en un nuevo archivo\n",
    "corrected_file_path = 'datasets/australian_users_items_corrected.json'\n",
    "with open(corrected_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(json_corrected)\n",
    "\n",
    "print(f\"Archivo JSON corregido guardado en: {corrected_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Ruta del archivo JSON original\n",
    "file_path = 'datasets/australian_users_items.json'\n",
    "\n",
    "# Leer el archivo con comillas simples\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_content = file.read()\n",
    "\n",
    "# Expresión regular para reemplazar comillas simples por comillas dobles en las claves y valores del JSON\n",
    "json_corrected = re.sub(r\"(?<!\\\\)'\", '\"', json_content)\n",
    "\n",
    "# Guardar el JSON corregido en un nuevo archivo\n",
    "corrected_file_path = 'datasets/australian_user_reviews_corrected.json'\n",
    "with open(corrected_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(json_corrected)\n",
    "\n",
    "print(f\"Archivo JSON corregido guardado en: {corrected_file_path}\")\n",
    "\n",
    "# Cargar el archivo JSON corregido\n",
    "with open(corrected_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Transformar la estructura\n",
    "reviews = []\n",
    "for user in data:\n",
    "    user_id = user.get('user_id')\n",
    "    user_url = user.get('user_url')\n",
    "    for review in user.get('reviews', []):\n",
    "        review_entry = {\n",
    "            'user_id': user_id,\n",
    "            'user_url': user_url,\n",
    "            'item_id': review.get('item_id'),\n",
    "            'recommend': review.get('recommend'),\n",
    "            'review': review.get('review'),\n",
    "            'funny': review.get('funny'),\n",
    "            'helpful': review.get('helpful'),\n",
    "            'posted': review.get('posted'),\n",
    "            'last_edited': review.get('last_edited'),\n",
    "        }\n",
    "        reviews.append(review_entry)\n",
    "\n",
    "# Guardar el nuevo JSON transformado\n",
    "transformed_file_path = 'transformed_reviews.json'\n",
    "with open(transformed_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(reviews, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Transformación completada y guardada en: {transformed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al evaluar el contenido JSON: invalid syntax (<unknown>, line 2)\n",
      "No se pudo corregir el JSON.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "# Ruta del archivo JSON original\n",
    "file_path = 'datasets/australian_user_reviews - copia.json'\n",
    "\n",
    "# Leer el archivo con comillas simples\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_content = file.read()\n",
    "\n",
    "# Usar ast.literal_eval para corregir las comillas\n",
    "try:\n",
    "    corrected_json_content = ast.literal_eval(json_content)\n",
    "except Exception as e:\n",
    "    print(f\"Error al evaluar el contenido JSON: {e}\")\n",
    "    corrected_json_content = None\n",
    "\n",
    "# Guardar el JSON corregido en un nuevo archivo si no hubo errores\n",
    "if corrected_json_content is not None:\n",
    "    corrected_file_path = 'datasets/australian_user_reviews_corrected.json'\n",
    "    with open(corrected_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(corrected_json_content, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Archivo JSON corregido guardado en: {corrected_file_path}\")\n",
    "\n",
    "    # Cargar el archivo JSON corregido\n",
    "    with open(corrected_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Transformar la estructura\n",
    "    reviews = []\n",
    "    for user in data:\n",
    "        user_id = user.get('user_id')\n",
    "        user_url = user.get('user_url')\n",
    "        for review in user.get('reviews', []):\n",
    "            review_entry = {\n",
    "                'user_id': user_id,\n",
    "                'user_url': user_url,\n",
    "                'item_id': review.get('item_id'),\n",
    "                'recommend': review.get('recommend'),\n",
    "                'review': review.get('review'),\n",
    "                'funny': review.get('funny'),\n",
    "                'helpful': review.get('helpful'),\n",
    "                'posted': review.get('posted'),\n",
    "                'last_edited': review.get('last_edited'),\n",
    "            }\n",
    "            reviews.append(review_entry)\n",
    "\n",
    "    # Guardar el nuevo JSON transformado\n",
    "    transformed_file_path = 'transformed_reviews.json'\n",
    "    with open(transformed_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(reviews, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Transformación completada y guardada en: {transformed_file_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No se pudo corregir el JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Cargar el archivo JSON original\n",
    "with open('australian_user_reviews', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Transformar la estructura\n",
    "reviews = []\n",
    "for user in data:\n",
    "    user_id = user.get('user_id')\n",
    "    user_url = user.get('user_url')\n",
    "    for review in user.get('reviews', []):\n",
    "        review_entry = {\n",
    "            'user_id': user_id,\n",
    "            'user_url': user_url,\n",
    "            'item_id': review.get('item_id'),\n",
    "            'recommend': review.get('recommend'),\n",
    "            'review': review.get('review'),\n",
    "            'funny': review.get('funny'),\n",
    "            'helpful': review.get('helpful'),\n",
    "            'posted': review.get('posted'),\n",
    "            'last_edited': review.get('last_edited'),\n",
    "        }\n",
    "        reviews.append(review_entry)\n",
    "\n",
    "# Guardar el nuevo JSON\n",
    "with open('transformed_reviews.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(reviews, file, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
